{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T21:14:05.801967Z",
     "start_time": "2025-01-09T21:13:29.509516Z"
    }
   },
   "source": [
    "import gspread\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def auth_gspread():\n",
    "    try:\n",
    "        gc = gspread.service_account(filename='gfile.json')\n",
    "        sheet = gc.open(\"CMACC_sheet\").sheet1\n",
    "        print(\"Successfully authenticated and accessed the sheet.\")\n",
    "        return sheet\n",
    "    except Exception as e:\n",
    "        print(f\"Error authenticating Google Sheets: {e}\")\n",
    "        raise\n",
    "\n",
    "def assign_result(row, all_days_df):\n",
    "    current_date = pd.to_datetime(row[\"Scrape Date\"]).date()\n",
    "    dog_id = row[\"ID\"]\n",
    "    all_days_df[\"Scrape Date\"] = pd.to_datetime(all_days_df[\"Scrape Date\"]).dt.date\n",
    "    most_recent_date = all_days_df[\"Scrape Date\"].max()\n",
    "    is_in_next = all_days_df[(all_days_df[\"Scrape Date\"] > current_date) & (all_days_df[\"ID\"] == dog_id)].shape[0] > 0\n",
    "    return \"Adopted\" if not is_in_next and current_date != most_recent_date else \"Looking for Furever Home\"\n",
    "\n",
    "def convert_age_to_years(age):\n",
    "    if pd.isnull(age):\n",
    "        return None\n",
    "    if isinstance(age, str):\n",
    "        if 'year' in age:\n",
    "            return int(age.split()[0])\n",
    "        elif 'month' in age:\n",
    "            return int(age.split()[0]) / 12\n",
    "        elif 'week' in age:\n",
    "            return int(age.split()[0]) / 52\n",
    "    return None\n",
    "\n",
    "def wrangle(dogs_scrape):\n",
    "    for col in ['Brought to Shelter', 'Scrape Date']:\n",
    "        dogs_scrape[col] = pd.to_datetime(dogs_scrape[col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    dogs_scrape['age_numeric'] = dogs_scrape['Age'].apply(convert_age_to_years)\n",
    "    dogs_scrape['Weight_num'] = dogs_scrape['Weight'].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "    dogs_scrape.rename(columns={'Name': 'Name_ID'}, inplace=True)\n",
    "    dogs_scrape['Name'] = dogs_scrape['Name_ID'].str.extract(r'([A-Za-z\\s]+) \\(')\n",
    "    dogs_scrape['ID'] = dogs_scrape['Name_ID'].str.extract(r'(\\d+)')\n",
    "    dogs_scrape[\"result\"] = dogs_scrape.apply(assign_result, axis=1, all_days_df=dogs_scrape)\n",
    "    return dogs_scrape\n",
    "\n",
    "def fetch_and_parse(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Request error occurred: {req_err}\")\n",
    "    return None\n",
    "\n",
    "def extract_data(soup):\n",
    "    dog_list = []\n",
    "    if soup:\n",
    "        dogs = soup.find_all('div', class_='gridResult')\n",
    "        for dog in dogs:\n",
    "            try:\n",
    "                dog_list.append({\n",
    "                    'Name': dog.find('span', class_='text_Name results').text.strip(),\n",
    "                    'Gender': dog.find('span', class_='text_Gender results').text.strip(),\n",
    "                    'Breed': dog.find('span', class_='text_Breed results').text.strip(),\n",
    "                    'Age': dog.find('span', class_='text_Age results').text.strip(),\n",
    "                    'Animal Type': dog.find('span', class_='text_Animaltype results').text.strip(),\n",
    "                    'Weight': dog.find('span', class_='text_Weight results').text.strip(),\n",
    "                    'Brought to Shelter': dog.find('span', class_='text_Broughttotheshelter results').text.strip(),\n",
    "                    'Kennel Location': dog.find('span', class_='text_KennelLocation results').text.strip()\n",
    "                })\n",
    "            except AttributeError as e:\n",
    "                print(f\"Error extracting data for one dog: {e}\")\n",
    "    return dog_list\n",
    "\n",
    "def scrape_dog_data():\n",
    "    base_url = 'https://24petconnect.com/CLTAdopt'\n",
    "    index = 0\n",
    "    all_dogs = []\n",
    "    while True:\n",
    "        current_url = f\"{base_url}?index={index}&at=DOG\"\n",
    "        print(f\"Fetching data from index: {index}\")\n",
    "        soup = fetch_and_parse(current_url)\n",
    "        if soup is None:\n",
    "            print(\"Error fetching page. Exiting loop.\")\n",
    "            break\n",
    "        new_dogs = extract_data(soup)\n",
    "        if not new_dogs:\n",
    "            print(\"No more dogs found. Exiting loop.\")\n",
    "            break\n",
    "        all_dogs.extend(new_dogs)\n",
    "        index += 30\n",
    "        time.sleep(1)\n",
    "    if all_dogs:\n",
    "        dogs_at_shelter = pd.DataFrame(all_dogs)\n",
    "        dogs_at_shelter['Scrape Date'] = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
    "        dogs_at_shelter = wrangle(dogs_at_shelter)\n",
    "        csv_file = 'dogs_at_shelter_test.csv'\n",
    "        if os.path.exists(csv_file):\n",
    "            existing_data = pd.read_csv(csv_file)\n",
    "            filtered_data = existing_data[existing_data['Scrape Date'] != pd.to_datetime('today').strftime('%Y-%m-%d')]\n",
    "            updated_data = pd.concat([filtered_data, dogs_at_shelter], ignore_index=True)\n",
    "            updated_data.to_csv(csv_file, index=False)\n",
    "            try:\n",
    "                sheet = auth_gspread()\n",
    "                sheet.clear()\n",
    "                for col in updated_data.columns:\n",
    "                    updated_data[col] = updated_data[col].astype(str)\n",
    "                sheet.update([updated_data.columns.values.tolist()] + updated_data.values.tolist())\n",
    "                print(\"Data saved to Google Sheet.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "        else:\n",
    "            dogs_at_shelter.to_csv(csv_file, index=False)\n",
    "            print(\"Data saved as new file.\")\n",
    "    else:\n",
    "        print(\"No data fetched today.\")\n",
    "\n",
    "scrape_dog_data()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from index: 0\n",
      "Fetching data from index: 30\n",
      "Fetching data from index: 60\n",
      "Fetching data from index: 90\n",
      "Fetching data from index: 120\n",
      "Fetching data from index: 150\n",
      "Fetching data from index: 180\n",
      "Fetching data from index: 210\n",
      "Fetching data from index: 240\n",
      "Fetching data from index: 270\n",
      "Fetching data from index: 300\n",
      "Fetching data from index: 330\n",
      "HTTP error occurred: 500 Server Error: Internal Server Error for url: https://24petconnect.com/CLTAdopt?index=330&at=DOG\n",
      "Error fetching page. Exiting loop.\n",
      "Successfully authenticated and accessed the sheet.\n",
      "Data saved to Google Sheet.\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T21:05:11.299664Z",
     "start_time": "2025-01-09T21:03:28.789145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import schedule\n",
    "\n",
    "# Scheduling\n",
    "def schedule_job():\n",
    "    schedule.every(30).minutes.do(scrape_dog_data)\n",
    "    print(\"Scheduled job to run every 30 mins...\")\n",
    "    try:\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Script interrupted by user, exiting.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrape_dog_data()\n",
    "    schedule_job()  # Then start the scheduled job\n",
    "\n"
   ],
   "id": "13809805db474eeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from index: 0\n",
      "Fetching data from index: 30\n",
      "Fetching data from index: 60\n",
      "Fetching data from index: 90\n",
      "Fetching data from index: 120\n",
      "Fetching data from index: 150\n",
      "Fetching data from index: 180\n",
      "Fetching data from index: 210\n",
      "Fetching data from index: 240\n",
      "Fetching data from index: 270\n",
      "Fetching data from index: 300\n",
      "Fetching data from index: 330\n",
      "HTTP error occurred: 500 Server Error: Internal Server Error for url: https://24petconnect.com/CLTAdopt?index=330&at=DOG\n",
      "Error fetching page. Exiting loop.\n",
      "Total dogs fetched: 309\n",
      "Existing data before filtering: (10815, 21)\n",
      "Data after filtering out today's date: (10506, 21)\n",
      "Data updated and saved.\n",
      "Successfully authenticated and accessed the sheet.\n",
      "Data saved to Google Sheet.\n",
      "Scheduled job to run every 30 mins...\n",
      "Script interrupted by user, exiting.\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7e097fae06cbe48a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
