{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T20:43:10.640721Z",
     "start_time": "2025-01-09T20:42:31.740422Z"
    }
   },
   "source": [
    "import gspread\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def auth_gspread():\n",
    "    try:\n",
    "        gc = gspread.service_account(filename='gfile.json')\n",
    "        sheet = gc.open(\"CMACC_sheet\").sheet1\n",
    "        print(\"Successfully authenticated and accessed the sheet.\")\n",
    "        return sheet\n",
    "    except Exception as e:\n",
    "        print(f\"Error authenticating Google Sheets: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to determine the adopted or looking\n",
    "def assign_result(row, all_days_df):\n",
    "    current_date = pd.to_datetime(row[\"Scrape Date\"]).date()\n",
    "    dog_id = row[\"ID\"]\n",
    "    # Convert Scrape Date column to datetime for efficient filtering\n",
    "    all_days_df[\"Scrape Date\"] = pd.to_datetime(all_days_df[\"Scrape Date\"]).dt.date\n",
    "    # Determine if this is the most recent date in the dataset\n",
    "    most_recent_date = all_days_df[\"Scrape Date\"].max()\n",
    "    # Check if the dog appears in any future data\n",
    "    is_in_next = all_days_df[\n",
    "        (all_days_df[\"Scrape Date\"] > current_date) & (all_days_df[\"ID\"] == dog_id)\n",
    "    ].shape[0] > 0\n",
    "    # Determine the result\n",
    "    if current_date == most_recent_date:\n",
    "        return \"Looking for Furever Home\"  # Default to Looking for Furever Home for the most recent date\n",
    "    elif not is_in_next:\n",
    "        return \"Adopted\"  # Not found in future dates -> Adopted\n",
    "    else:\n",
    "        return \"Looking for Furever Home\"  # Found in future dates\n",
    "\n",
    "# Convert age to years function\n",
    "def convert_age_to_years(age):\n",
    "    if pd.isnull(age):\n",
    "        return None\n",
    "    if isinstance(age, str):\n",
    "        if 'year' in age:\n",
    "            return int(age.split()[0])\n",
    "        elif 'month' in age:\n",
    "            return int(age.split()[0]) / 12\n",
    "        elif 'week' in age:\n",
    "            return int(age.split()[0]) / 52\n",
    "    return None\n",
    "\n",
    "def wrangle(dogs_scrape):\n",
    "    date_columns = ['Brought to Shelter', 'Scrape Date']  # List all your date columns here\n",
    "    for col in date_columns:\n",
    "        if col in dogs_scrape.columns:\n",
    "            dogs_scrape[col] = pd.to_datetime(dogs_scrape[col]).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Convert age and extract numerical values from strings\n",
    "    dogs_scrape['age_numeric'] = dogs_scrape['Age'].apply(convert_age_to_years)\n",
    "    dogs_scrape['Weight_num'] = dogs_scrape['Weight'].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "    # Rename columns and split values\n",
    "    dogs_scrape.rename(columns={'Name': 'Name_ID'}, inplace=True)\n",
    "    dogs_scrape['Name'] = dogs_scrape['Name_ID'].str.extract(r'([A-Za-z\\s]+) \\(')\n",
    "    dogs_scrape['ID'] = dogs_scrape['Name_ID'].str.extract(r'(\\d+)')\n",
    "\n",
    "    # Simplify location descriptions\n",
    "    dogs_scrape['location_simple'] = np.where(\n",
    "        dogs_scrape['Kennel Location'].str.contains('toom', case=False, na=False), 'Toomey',\n",
    "        np.where(\n",
    "            dogs_scrape['Kennel Location'].str.contains('CARE|LST|ADOPT|PUPPY|INTAKE|CLINIC|FOUND|FERRET|ADPT', case=False), 'Byrum',\n",
    "        np.where(\n",
    "            dogs_scrape['Kennel Location'].str.contains('Foster', case=False), 'Foster',\n",
    "            dogs_scrape['Kennel Location'])))\n",
    "\n",
    "    # Extract components from Kennel Location\n",
    "    dogs_scrape['kennel_name'] = dogs_scrape['Kennel Location'].str.extract(r'([A-Za-z\\s]+)')\n",
    "    dogs_scrape['kennel_num'] = dogs_scrape['Kennel Location'].str.extract(r'(\\d+)')\n",
    "    dogs_scrape['side'] = dogs_scrape['Kennel Location'].apply(lambda x: '' if 'FOSTER' in x or x[-1] not in ['R', 'L'] else x[-1])\n",
    "\n",
    "    # Ensure date fields are correctly converted to datetime objects and handle 'NaT' values\n",
    "    dogs_scrape['Brought to Shelter'] = pd.to_datetime(dogs_scrape['Brought to Shelter'], errors='coerce').dt.date\n",
    "    dogs_scrape['Scrape Date'] = pd.to_datetime(dogs_scrape['Scrape Date'], errors='coerce').dt.date\n",
    "\n",
    "    # Calculate days at shelter safely\n",
    "    for index, row in dogs_scrape.iterrows():\n",
    "        if pd.notna(row['Brought to Shelter']) and pd.notna(row['Scrape Date']):\n",
    "            delta = row['Scrape Date'] - row['Brought to Shelter']\n",
    "            dogs_scrape.at[index, 'Days_at_shelter'] = delta.days\n",
    "        else:\n",
    "            dogs_scrape.at[index, 'Days_at_shelter'] = None  # Assign None if either date is NaT\n",
    "\n",
    "    # Apply function to assign results based on dates\n",
    "    dogs_scrape[\"result\"] = dogs_scrape.apply(assign_result, axis=1, all_days_df=dogs_scrape)\n",
    "    return dogs_scrape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to fetch and parse HTML\n",
    "def fetch_and_parse(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Request error occurred: {req_err}\")\n",
    "    return None\n",
    "\n",
    "# Function to extract data from soup\n",
    "def extract_data(soup):\n",
    "    if soup is None:\n",
    "        return []\n",
    "    dogs = soup.find_all('div', class_='gridResult')\n",
    "    dog_list = []\n",
    "    for dog in dogs:\n",
    "        try:\n",
    "            dog_list.append({\n",
    "                'Name': dog.find('span', class_='text_Name results').text.strip() if dog.find('span', class_='text_Name results') else 'N/A',\n",
    "                'Gender': dog.find('span', class_='text_Gender results').text.strip() if dog.find('span', class_='text_Gender results') else 'N/A',\n",
    "                'Breed': dog.find('span', class_='text_Breed results').text.strip() if dog.find('span', class_='text_Breed results') else 'N/A',\n",
    "                'Age': dog.find('span', class_='text_Age results').text.strip() if dog.find('span', class_='text_Age results') else 'N/A',\n",
    "                'Animal Type': dog.find('span', class_='text_Animaltype results').text.strip() if dog.find('span', class_='text_Animaltype results') else 'N/A',\n",
    "                'Weight': dog.find('span', class_='text_Weight results').text.strip() if dog.find('span', class_='text_Weight results') else 'N/A',\n",
    "                'Brought to Shelter': dog.find('span', class_='text_Broughttotheshelter results').text.strip() if dog.find('span', class_='text_Broughttotheshelter results') else 'N/A',\n",
    "                'Located At': dog.find('span', class_='text_Locatedat results').text.strip() if dog.find('span', class_='text_Locatedat results') else '',\n",
    "                'Kennel Location': dog.find('span', class_='text_KennelLocation results').text.strip() if dog.find('span', class_='text_KennelLocation results') else '',\n",
    "                'Qualified For': dog.find('span', class_='text_ViewType results').text.strip() if dog.find('span', class_='text_ViewType results') else 'N/A'\n",
    "            })\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error extracting data for one dog: {e}\")\n",
    "    return dog_list\n",
    "\n",
    "# Main function to scrape dog data\n",
    "def scrape_dog_data():\n",
    "    base_url = 'https://24petconnect.com/CLTAdopt'\n",
    "    index = 0\n",
    "    all_dogs = []\n",
    "    current_date = pd.to_datetime(datetime.datetime.now().strftime('%Y-%m-%d')).date()\n",
    "\n",
    "    while True:\n",
    "        current_url = f\"{base_url}?index={index}&at=DOG\"\n",
    "        print(f\"Fetching data from index: {index}\")\n",
    "        soup = fetch_and_parse(current_url)\n",
    "\n",
    "        if soup is None:\n",
    "            print(\"Error fetching page. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        new_dogs = extract_data(soup)\n",
    "        if not new_dogs:\n",
    "            print(\"No more dogs found. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        all_dogs.extend(new_dogs)\n",
    "        index += 30\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(f\"Total dogs fetched: {len(all_dogs)}\")\n",
    "\n",
    "    if all_dogs:\n",
    "        dogs_at_shelter = pd.DataFrame(all_dogs)\n",
    "        dogs_at_shelter['Scrape Date'] = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
    "        dogs_at_shelter = wrangle(dogs_at_shelter)\n",
    "\n",
    "        csv_file = 'dogs_at_shelter_test.csv'\n",
    "        if os.path.exists(csv_file):\n",
    "            existing_data = pd.read_csv(csv_file)\n",
    "            existing_data['Scrape Date'] = pd.to_datetime(existing_data['Scrape Date']).dt.strftime('%Y-%m-%d')\n",
    "            print(f\"Existing data before filtering: {existing_data.shape}\")\n",
    "\n",
    "            filtered_data = existing_data[existing_data['Scrape Date'] != current_date.strftime('%Y-%m-%d')]\n",
    "            print(f\"Data after filtering out today's date: {filtered_data.shape}\")\n",
    "\n",
    "            updated_data = pd.concat([filtered_data, dogs_at_shelter], ignore_index=True)\n",
    "            updated_data['Scrape Date'] = updated_data['Scrape Date'].astype(str)  # Ensure Scrape Date is a string\n",
    "            updated_data.to_csv(csv_file, index=False)\n",
    "            print(\"Data updated and saved.\")\n",
    "            try:\n",
    "                sheet = auth_gspread()\n",
    "                sheet.clear()\n",
    "                # Ensure all data to upload is in string format\n",
    "                for col in updated_data.columns:\n",
    "                    updated_data[col] = updated_data[col].astype(str)\n",
    "\n",
    "                sheet.update([updated_data.columns.values.tolist()] + updated_data.values.tolist())\n",
    "                print(\"Data saved to Google Sheet.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating Google Sheet: {e}\")\n",
    "        else:\n",
    "            dogs_at_shelter.to_csv(csv_file, index=False)\n",
    "            print(\"Data saved as new file.\")\n",
    "    else:\n",
    "        print(\"No data fetched today.\")\n",
    "\n",
    "scrape_dog_data()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from index: 0\n",
      "Fetching data from index: 30\n",
      "Fetching data from index: 60\n",
      "Fetching data from index: 90\n",
      "Fetching data from index: 120\n",
      "Fetching data from index: 150\n",
      "Fetching data from index: 180\n",
      "Fetching data from index: 210\n",
      "Fetching data from index: 240\n",
      "Fetching data from index: 270\n",
      "Fetching data from index: 300\n",
      "Fetching data from index: 330\n",
      "HTTP error occurred: 500 Server Error: Internal Server Error for url: https://24petconnect.com/CLTAdopt?index=330&at=DOG\n",
      "Error fetching page. Exiting loop.\n",
      "Total dogs fetched: 310\n",
      "Existing data before filtering: (10596, 21)\n",
      "Data after filtering out today's date: (10506, 21)\n",
      "Data updated and saved.\n",
      "Successfully authenticated and accessed the sheet.\n",
      "Data saved to Google Sheet.\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-09T20:43:10.699850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import schedule\n",
    "\n",
    "# Scheduling\n",
    "def schedule_job():\n",
    "    schedule.every(30).minutes.do(scrape_dog_data)\n",
    "    print(\"Scheduled job to run every 30 mins...\")\n",
    "    try:\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Script interrupted by user, exiting.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    schedule_job()  # Then start the scheduled job\n",
    "\n"
   ],
   "id": "13809805db474eeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled job to run every 30 mins...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7e097fae06cbe48a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
